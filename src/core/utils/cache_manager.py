"""
diskcache ê¸°ë°˜ ìºì‹œ ë° ìŠ¤ëƒ…ìƒ· ê´€ë¦¬ì
Redis-JSON ê¸°ëŠ¥ì„ diskcacheë¡œ ëŒ€ì²´

PRD ìš”êµ¬ì‚¬í•­ì— ë”°ë¥¸ ìºì‹œ ë° ì²´í¬í¬ì¸íŠ¸ ì €ì¥/ì¡°íšŒ ê¸°ëŠ¥
"""

import json
import logging
import re
from datetime import datetime, timedelta
from typing import Optional, Dict, Any, List, Union
from pathlib import Path
import diskcache
from pydantic import BaseModel

# ê¸°ì¡´ ìŠ¤í‚¤ë§ˆ import
from ..schemas.base import WorkflowState, CheckpointData, CheckpointType

logger = logging.getLogger(__name__)


class CacheConfig(BaseModel):
    """ìºì‹œ ì„¤ì •"""
    cache_dir: str = "./data/cache"
    max_size: int = 1024 * 1024 * 1024  # 1GB
    eviction_policy: str = "least-recently-used"
    default_ttl: int = 86400  # 24ì‹œê°„
    checkpoint_ttl: int = 604800  # 7ì¼
    
    @classmethod
    def from_env(cls):
        """í™˜ê²½ë³€ìˆ˜ì—ì„œ ì„¤ì • ë¡œë“œ"""
        import os
        return cls(
            cache_dir=os.getenv("CACHE_DIR", "./data/cache"),
            max_size=int(os.getenv("CACHE_MAX_SIZE", "1073741824")),
            eviction_policy=os.getenv("CACHE_EVICTION_POLICY", "least-recently-used"),
            default_ttl=int(os.getenv("CACHE_DEFAULT_TTL", "86400")),
            checkpoint_ttl=int(os.getenv("CACHE_CHECKPOINT_TTL", "604800"))
        )


class CacheManager:
    """
    diskcache ê¸°ë°˜ ìºì‹œ ê´€ë¦¬ì
    Redis-JSON ê¸°ëŠ¥ì„ ë¡œì»¬ ìºì‹œë¡œ ëŒ€ì²´
    """
    
    def __init__(self, config: CacheConfig):
        self.config = config
        self.cache_dir = Path(config.cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        # ë©”ì¸ ìºì‹œ (ì¼ë°˜ ë°ì´í„°)
        self.cache = diskcache.Cache(
            directory=str(self.cache_dir / "main"),
            size_limit=config.max_size,
            eviction_policy=config.eviction_policy
        )
        
        # ì²´í¬í¬ì¸íŠ¸ ìºì‹œ (ì¤‘ìš” ë°ì´í„°, ë” ê¸´ TTL)
        self.checkpoint_cache = diskcache.Cache(
            directory=str(self.cache_dir / "checkpoints"),
            size_limit=config.max_size // 2,
            eviction_policy="least-recently-used"
        )
        
        logger.info(f"CacheManager initialized with directory: {self.cache_dir}")
    
    # Redis-JSON í˜¸í™˜ ë©”ì„œë“œë“¤
    def json_set(self, key: str, path: str, value: Any, ttl: Optional[int] = None) -> bool:
        """
        Redis JSON.SET í˜¸í™˜ ë©”ì„œë“œ
        
        Args:
            key: ìºì‹œ í‚¤
            path: JSON ê²½ë¡œ (ì˜ˆ: "$", "$.field")
            value: ì €ì¥í•  ê°’
            ttl: ë§Œë£Œ ì‹œê°„ (ì´ˆ)
        
        Returns:
            bool: ì €ì¥ ì„±ê³µ ì—¬ë¶€
        """
        try:
            if path == "$":
                # ì „ì²´ ê°ì²´ ì„¤ì •
                expire_time = ttl or self.config.default_ttl
                return self.cache.set(key, value, expire=expire_time)
            else:
                # ê²½ë¡œë³„ ì„¤ì •
                current = self.cache.get(key, {})
                self._set_json_path(current, path, value)
                expire_time = ttl or self.config.default_ttl
                return self.cache.set(key, current, expire=expire_time)
        except Exception as e:
            logger.error(f"JSON set failed for key '{key}', path '{path}': {e}")
            return False
    
    def json_get(self, key: str, path: str = "$") -> Any:
        """
        Redis JSON.GET í˜¸í™˜ ë©”ì„œë“œ
        
        Args:
            key: ìºì‹œ í‚¤
            path: JSON ê²½ë¡œ
        
        Returns:
            Any: ì¡°íšŒëœ ê°’ ë˜ëŠ” None
        """
        try:
            data = self.cache.get(key)
            if data is None:
                return None
            
            if path == "$":
                return data
            else:
                return self._get_json_path(data, path)
        except Exception as e:
            logger.error(f"JSON get failed for key '{key}', path '{path}': {e}")
            return None
    
    def json_del(self, key: str, path: str = "$") -> bool:
        """
        Redis JSON.DEL í˜¸í™˜ ë©”ì„œë“œ
        
        Args:
            key: ìºì‹œ í‚¤
            path: JSON ê²½ë¡œ
        
        Returns:
            bool: ì‚­ì œ ì„±ê³µ ì—¬ë¶€
        """
        try:
            if path == "$":
                # ì „ì²´ í‚¤ ì‚­ì œ
                return self.cache.delete(key)
            else:
                # ê²½ë¡œë³„ ì‚­ì œ
                data = self.cache.get(key)
                if data is None:
                    return False
                
                if self._delete_json_path(data, path):
                    return self.cache.set(key, data)
                return False
        except Exception as e:
            logger.error(f"JSON delete failed for key '{key}', path '{path}': {e}")
            return False
    
    # ì²´í¬í¬ì¸íŠ¸ íŠ¹í™” ë©”ì„œë“œë“¤
    def save_checkpoint(self, workflow_id: str, checkpoint_data: CheckpointData) -> bool:
        """
        ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        
        Args:
            workflow_id: ì›Œí¬í”Œë¡œìš° ID
            checkpoint_data: ì²´í¬í¬ì¸íŠ¸ ë°ì´í„°
        
        Returns:
            bool: ì €ì¥ ì„±ê³µ ì—¬ë¶€
        """
        try:
            # ê¸°ì¡´ redis_manager.pyì™€ í˜¸í™˜ë˜ëŠ” í‚¤ í¬ë§· ì‚¬ìš©
            key = f"kg_checkpoint:{workflow_id}:{checkpoint_data.checkpoint_type}:{checkpoint_data.timestamp.isoformat()}"
            
            # Pydantic ëª¨ë¸ì„ dictë¡œ ë³€í™˜
            data = checkpoint_data.dict()
            
            success = self.checkpoint_cache.set(
                key, 
                data, 
                expire=self.config.checkpoint_ttl
            )
            
            if success:
                logger.info(f"Checkpoint saved: {key}")
            return success
            
        except Exception as e:
            logger.error(f"Checkpoint save failed for workflow '{workflow_id}': {e}")
            return False
    
    def get_checkpoint(self, workflow_id: str, checkpoint_type: Optional[str] = None) -> Optional[CheckpointData]:
        """
        ìµœì‹  ì²´í¬í¬ì¸íŠ¸ ì¡°íšŒ
        
        Args:
            workflow_id: ì›Œí¬í”Œë¡œìš° ID
            checkpoint_type: ì²´í¬í¬ì¸íŠ¸ íƒ€ì… (Noneì´ë©´ ìµœì‹  ê²ƒ)
        
        Returns:
            Optional[CheckpointData]: ì²´í¬í¬ì¸íŠ¸ ë°ì´í„° ë˜ëŠ” None
        """
        try:
            pattern = f"kg_checkpoint:{workflow_id}:"
            if checkpoint_type:
                pattern += f"{checkpoint_type}:"
            
            # diskcacheì—ì„œ íŒ¨í„´ ë§¤ì¹­ (ìˆ˜ë™ êµ¬í˜„)
            matching_keys = [k for k in self.checkpoint_cache if str(k).startswith(pattern)]
            if not matching_keys:
                return None
            
            # ìµœì‹  í‚¤ ì„ íƒ (íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ì¤€)
            latest_key = max(matching_keys, key=lambda k: str(k))
            data = self.checkpoint_cache.get(latest_key)
            
            if data:
                # dictë¥¼ CheckpointDataë¡œ ë³µì›
                return CheckpointData(**data)
            return None
            
        except Exception as e:
            logger.error(f"Checkpoint get failed for workflow '{workflow_id}': {e}")
            return None
    
    def get_checkpoints_by_workflow(self, workflow_id: str, limit: int = 10) -> List[CheckpointData]:
        """
        ì›Œí¬í”Œë¡œìš°ë³„ ì²´í¬í¬ì¸íŠ¸ ëª©ë¡ ì¡°íšŒ
        
        Args:
            workflow_id: ì›Œí¬í”Œë¡œìš° ID
            limit: ìµœëŒ€ ì¡°íšŒ ê°œìˆ˜
        
        Returns:
            List[CheckpointData]: ì²´í¬í¬ì¸íŠ¸ ëª©ë¡
        """
        try:
            pattern = f"kg_checkpoint:{workflow_id}:"
            matching_keys = []
            
            # ì¼ë°˜ ìºì‹œì™€ ì²´í¬í¬ì¸íŠ¸ ìºì‹œ ëª¨ë‘ í™•ì¸
            for cache in [self.cache, self.checkpoint_cache]:
                for key in cache:
                    key_str = str(key)
                    if key_str.startswith(pattern) and not key_str.endswith(":latest"):
                        matching_keys.append(key_str)
            
            logger.debug(f"Found {len(matching_keys)} matching keys for workflow {workflow_id}")
            
            # íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ì¤€ ì •ë ¬ (ìµœì‹ ìˆœ)
            sorted_keys = sorted(matching_keys, key=lambda k: str(k), reverse=True)[:limit]
            
            checkpoints = []
            for key in sorted_keys:
                # ë‘ ìºì‹œì—ì„œ ëª¨ë‘ ì‹œë„
                data = self.checkpoint_cache.get(key) or self.cache.get(key)
                if data:
                    try:
                        if isinstance(data, str):
                            # JSON ë¬¸ìì—´ì¸ ê²½ìš°
                            checkpoint = CheckpointData.model_validate_json(data)
                        else:
                            # ì´ë¯¸ dictì¸ ê²½ìš°
                            checkpoint = CheckpointData(**data)
                        checkpoints.append(checkpoint)
                    except Exception as e:
                        logger.warning(f"Failed to parse checkpoint data for key {key}: {e}")
                        continue
            
            logger.info(f"Retrieved {len(checkpoints)} checkpoints for workflow {workflow_id}")
            return checkpoints
            
        except Exception as e:
            logger.error(f"Get checkpoints failed for workflow '{workflow_id}': {e}")
            return []
    
    def delete_checkpoint(self, checkpoint_id: str) -> bool:
        """
        ì²´í¬í¬ì¸íŠ¸ ì‚­ì œ
        
        Args:
            checkpoint_id: ì²´í¬í¬ì¸íŠ¸ ID
        
        Returns:
            bool: ì‚­ì œ ì„±ê³µ ì—¬ë¶€
        """
        try:
            # ì²´í¬í¬ì¸íŠ¸ IDë¡œ í‚¤ ì°¾ê¸°
            for key in self.checkpoint_cache:
                data = self.checkpoint_cache.get(key)
                if data and data.get('checkpoint_id') == checkpoint_id:
                    return self.checkpoint_cache.delete(key)
            return False
            
        except Exception as e:
            logger.error(f"Checkpoint delete failed for ID '{checkpoint_id}': {e}")
            return False
    
    def cleanup_expired_checkpoints(self) -> int:
        """
        ë§Œë£Œëœ ì²´í¬í¬ì¸íŠ¸ ì •ë¦¬
        
        Returns:
            int: ì •ë¦¬ëœ ì²´í¬í¬ì¸íŠ¸ ìˆ˜
        """
        try:
            # diskcacheëŠ” ìë™ìœ¼ë¡œ TTLì„ ì²˜ë¦¬í•˜ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ìˆ˜ë™ ì •ë¦¬ ì—†ìŒ
            # ëŒ€ì‹  í†µê³„ë§Œ ë°˜í™˜
            return 0
        except Exception as e:
            logger.error(f"Cleanup failed: {e}")
            return 0
    
    # ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œë“¤
    def _set_json_path(self, obj: Dict, path: str, value: Any):
        """JSON ê²½ë¡œì— ê°’ ì„¤ì •"""
        if path.startswith("$."):
            keys = path[2:].split(".")
            current = obj
            for key in keys[:-1]:
                if key not in current:
                    current[key] = {}
                current = current[key]
            current[keys[-1]] = value
    
    def _get_json_path(self, obj: Any, path: str) -> Any:
        """JSON ê²½ë¡œì—ì„œ ê°’ ì¡°íšŒ"""
        if path.startswith("$."):
            keys = path[2:].split(".")
            current = obj
            for key in keys:
                if isinstance(current, dict) and key in current:
                    current = current[key]
                else:
                    return None
            return current
        return obj
    
    def _delete_json_path(self, obj: Dict, path: str) -> bool:
        """JSON ê²½ë¡œì˜ ê°’ ì‚­ì œ"""
        if path.startswith("$."):
            keys = path[2:].split(".")
            current = obj
            for key in keys[:-1]:
                if isinstance(current, dict) and key in current:
                    current = current[key]
                else:
                    return False
            
            if isinstance(current, dict) and keys[-1] in current:
                del current[keys[-1]]
                return True
        return False
    
    # ì¼ë°˜ ìºì‹œ ë©”ì„œë“œë“¤
    def set(self, key: str, value: Any, ttl: Optional[int] = None) -> bool:
        """ì¼ë°˜ ìºì‹œ ì €ì¥"""
        try:
            expire_time = ttl or self.config.default_ttl
            return self.cache.set(key, value, expire=expire_time)
        except Exception as e:
            logger.error(f"Cache set failed for key '{key}': {e}")
            return False
    
    def get(self, key: str, default: Any = None) -> Any:
        """ì¼ë°˜ ìºì‹œ ì¡°íšŒ"""
        try:
            return self.cache.get(key, default)
        except Exception as e:
            logger.error(f"Cache get failed for key '{key}': {e}")
            return default
    
    def delete(self, key: str) -> bool:
        """ì¼ë°˜ ìºì‹œ ì‚­ì œ"""
        try:
            return self.cache.delete(key)
        except Exception as e:
            logger.error(f"Cache delete failed for key '{key}': {e}")
            return False
    
    def clear(self) -> bool:
        """ì „ì²´ ìºì‹œ ì •ë¦¬"""
        try:
            self.cache.clear()
            self.checkpoint_cache.clear()
            return True
        except Exception as e:
            logger.error(f"Cache clear failed: {e}")
            return False
    
    def health_check(self) -> Dict[str, Any]:
        """ìƒíƒœ ì ê²€"""
        try:
            # ê°„ë‹¨í•œ write/read í…ŒìŠ¤íŠ¸
            test_key = "health_check_test"
            test_value = {"timestamp": datetime.utcnow().isoformat()}
            
            # ë©”ì¸ ìºì‹œ í…ŒìŠ¤íŠ¸
            main_write = self.cache.set(test_key, test_value, expire=10)
            main_read = self.cache.get(test_key) if main_write else None
            self.cache.delete(test_key)
            
            # ì²´í¬í¬ì¸íŠ¸ ìºì‹œ í…ŒìŠ¤íŠ¸
            checkpoint_write = self.checkpoint_cache.set(test_key, test_value, expire=10)
            checkpoint_read = self.checkpoint_cache.get(test_key) if checkpoint_write else None
            self.checkpoint_cache.delete(test_key)
            
            return {
                "status": "healthy" if (main_read and checkpoint_read) else "error",
                "cache_stats": {
                    "main_cache_size": len(self.cache),
                    "checkpoint_cache_size": len(self.checkpoint_cache),
                    "main_disk_usage": self.cache.volume(),
                    "checkpoint_disk_usage": self.checkpoint_cache.volume(),
                    "total_disk_usage": self.cache.volume() + self.checkpoint_cache.volume()
                },
                "config": self.config.dict(),
                "tests": {
                    "main_cache_rw": main_read is not None,
                    "checkpoint_cache_rw": checkpoint_read is not None
                }
            }
        except Exception as e:
            return {"status": "error", "error": str(e)}


if __name__ == "__main__":
    # ê¸°ë³¸ í…ŒìŠ¤íŠ¸
    config = CacheConfig()
    manager = CacheManager(config)
    print("âœ… CacheManager ì™„ì „ êµ¬í˜„ ì„±ê³µ")
    print("ğŸ“Š ìƒíƒœ:", json.dumps(manager.health_check(), indent=2))