"""
í†µí•© ì €ì¥ì†Œ ê´€ë¦¬ì
Redis ê¸°ëŠ¥ì„ diskcache + fakeredisë¡œ ëŒ€ì²´

ê¸°ì¡´ redis_manager.pyì˜ RedisManager + SnapshotManager ê¸°ëŠ¥ì„ ì™„ì „ ëŒ€ì²´
"""

import json
import asyncio
import logging
from typing import Optional, Dict, Any, List, Tuple
from datetime import datetime, timedelta
from pydantic import BaseModel

from .cache_manager import CacheManager, CacheConfig
from .lock_manager import DistributedLockManager
from ..schemas.base import WorkflowState, CheckpointData, CheckpointType

# ê°œë°œ/í…ŒìŠ¤íŠ¸ìš© Redis í˜¸í™˜ í´ë¼ì´ì–¸íŠ¸
try:
    import fakeredis
    FAKEREDIS_AVAILABLE = True
except ImportError:
    FAKEREDIS_AVAILABLE = False

logger = logging.getLogger(__name__)


class RedisConfig(BaseModel):
    """
    ê¸°ì¡´ RedisConfig í˜¸í™˜ì„±ì„ ìœ„í•œ í´ë˜ìŠ¤
    ì‹¤ì œë¡œëŠ” CacheConfigë¡œ ë³€í™˜ë¨
    """
    host: str = "localhost"
    port: int = 6379
    db: int = 0
    password: Optional[str] = None
    max_connections: int = 10
    socket_timeout: float = 5.0
    socket_connect_timeout: float = 5.0
    retry_on_timeout: bool = True
    
    def to_cache_config(self) -> CacheConfig:
        """CacheConfigë¡œ ë³€í™˜"""
        return CacheConfig(
            cache_dir="./data/cache",
            max_size=1024 * 1024 * 1024,  # 1GB
            eviction_policy="least-recently-used",
            default_ttl=86400,  # 24ì‹œê°„
            checkpoint_ttl=604800  # 7ì¼
        )


class RedisManager:
    """
    ê¸°ì¡´ RedisManager í˜¸í™˜ í´ë˜ìŠ¤
    ì‹¤ì œë¡œëŠ” CacheManager + FakeRedisë¥¼ ì‚¬ìš©
    """
    
    def __init__(self, config: RedisConfig):
        self.config = config
        self.cache_config = config.to_cache_config()
        self.cache_manager = CacheManager(self.cache_config)
        
        # FakeRedis í´ë¼ì´ì–¸íŠ¸ (í˜¸í™˜ì„±ìš©)
        if FAKEREDIS_AVAILABLE:
            self._sync_client = fakeredis.FakeRedis(decode_responses=True)
            self._async_client = None  # í•„ìš”ì‹œ êµ¬í˜„
        else:
            self._sync_client = None
            self._async_client = None
        
        logger.info("RedisManager (compatibility) initialized with diskcache backend")
    
    def get_sync_client(self):
        """ë™ê¸° Redis í´ë¼ì´ì–¸íŠ¸ ë°˜í™˜ (FakeRedis ë˜ëŠ” None)"""
        return self._sync_client
    
    async def get_async_client(self):
        """ë¹„ë™ê¸° Redis í´ë¼ì´ì–¸íŠ¸ ë°˜í™˜"""
        if self._async_client is None and FAKEREDIS_AVAILABLE:
            # FakeRedisëŠ” ë¹„ë™ê¸°ë¥¼ ì§€ì›í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ë™ê¸° í´ë¼ì´ì–¸íŠ¸ ë°˜í™˜
            return self._sync_client
        return self._async_client
    
    def test_connection(self) -> bool:
        """ì—°ê²° í…ŒìŠ¤íŠ¸"""
        try:
            if self._sync_client:
                self._sync_client.ping()
                return True
            # diskcache í…ŒìŠ¤íŠ¸
            return self.cache_manager.health_check()["status"] == "healthy"
        except Exception as e:
            logger.error(f"Connection test failed: {e}")
            return False
    
    async def test_async_connection(self) -> bool:
        """ë¹„ë™ê¸° ì—°ê²° í…ŒìŠ¤íŠ¸"""
        return self.test_connection()
    
    def close(self):
        """ì—°ê²° ì¢…ë£Œ"""
        # diskcacheëŠ” ìë™ìœ¼ë¡œ ì •ë¦¬ë¨
        pass
    
    async def close_async(self):
        """ë¹„ë™ê¸° ì—°ê²° ì¢…ë£Œ"""
        self.close()


class SnapshotManager:
    """
    ê¸°ì¡´ SnapshotManager ì™„ì „ í˜¸í™˜ í´ë˜ìŠ¤
    ë‚´ë¶€ì ìœ¼ë¡œ CacheManagerë¥¼ ì‚¬ìš©
    """
    
    def __init__(self, redis_manager: RedisManager):
        self.redis_manager = redis_manager
        self.cache_manager = redis_manager.cache_manager
        self.key_prefix = "kg_checkpoint"
        self.default_ttl = 86400 * 7  # 7ì¼
        
        logger.info("SnapshotManager initialized with diskcache backend")
    
    def _generate_key(self, trace_id: str, checkpoint_type: str = "snapshot") -> str:
        """ìŠ¤ëƒ…ìƒ· í‚¤ ìƒì„± (ê¸°ì¡´ í¬ë§· ìœ ì§€)"""
        timestamp = datetime.utcnow().strftime("%Y%m%d_%H%M%S")
        return f"{self.key_prefix}:{trace_id}:{checkpoint_type}:{timestamp}"
    
    def _generate_latest_key(self, trace_id: str) -> str:
        """ìµœì‹  ìŠ¤ëƒ…ìƒ· í‚¤ ìƒì„±"""
        return f"{self.key_prefix}:{trace_id}:latest"
    
    def save_checkpoint(
        self,
        checkpoint_data: CheckpointData,
        ttl: Optional[int] = None
    ) -> str:
        """
        ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (ë™ê¸°) - ê¸°ì¡´ API ì™„ì „ í˜¸í™˜
        """
        try:
            # ê¸°ì¡´ RedisManager ë°©ì‹ê³¼ ë™ì¼í•œ í‚¤ ìƒì„±
            key = self._generate_key(
                checkpoint_data.workflow_id, 
                checkpoint_data.checkpoint_type
            )
            latest_key = self._generate_latest_key(checkpoint_data.workflow_id)
            
            # JSON ì§ë ¬í™” (ê¸°ì¡´ ë°©ì‹ê³¼ ë™ì¼)
            data = checkpoint_data.model_dump_json()
            
            # CacheManagerë¥¼ í†µí•´ ì €ì¥
            expire_time = ttl or self.default_ttl
            self.cache_manager.set(key, data, ttl=expire_time)
            self.cache_manager.set(latest_key, key, ttl=expire_time)
            
            logger.info(f"ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì™„ë£Œ: {key}")
            return key
            
        except Exception as e:
            logger.error(f"ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì‹¤íŒ¨: {e}")
            raise
    
    async def save_checkpoint_async(
        self,
        checkpoint_data: CheckpointData,
        ttl: Optional[int] = None
    ) -> str:
        """
        ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (ë¹„ë™ê¸°) - ë™ê¸° ë²„ì „ í˜¸ì¶œ
        """
        return self.save_checkpoint(checkpoint_data, ttl)
    
    def load_checkpoint(self, workflow_id: str, latest: bool = True) -> Optional[CheckpointData]:
        """
        ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ (ë™ê¸°) - ê¸°ì¡´ API ì™„ì „ í˜¸í™˜
        """
        try:
            if latest:
                # ìµœì‹  ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
                latest_key = self._generate_latest_key(workflow_id)
                actual_key = self.cache_manager.get(latest_key)
                
                if not actual_key:
                    logger.warning(f"ì›Œí¬í”Œë¡œìš° {workflow_id}ì˜ ì²´í¬í¬ì¸íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                    return None
                
                data = self.cache_manager.get(actual_key)
                if not data:
                    logger.warning(f"ì²´í¬í¬ì¸íŠ¸ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {actual_key}")
                    return None
                
                return CheckpointData.model_validate_json(data)
            else:
                # ëª¨ë“  ì²´í¬í¬ì¸íŠ¸ ëª©ë¡ - CacheManager ë°©ì‹ìœ¼ë¡œ êµ¬í˜„
                return self._load_all_checkpoints_for_workflow(workflow_id)[0] if self._load_all_checkpoints_for_workflow(workflow_id) else None
                
        except Exception as e:
            logger.error(f"ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    async def load_checkpoint_async(
        self, 
        workflow_id: str, 
        latest: bool = True
    ) -> Optional[CheckpointData]:
        """
        ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ (ë¹„ë™ê¸°) - ë™ê¸° ë²„ì „ í˜¸ì¶œ
        """
        return self.load_checkpoint(workflow_id, latest)
    
    def save_workflow_state(
        self,
        workflow_state: WorkflowState,
        checkpoint_type: str = "periodic"
    ) -> str:
        """
        ì›Œí¬í”Œë¡œìš° ìƒíƒœ ìŠ¤ëƒ…ìƒ· ì €ì¥ - ê¸°ì¡´ API ì™„ì „ í˜¸í™˜
        """
        checkpoint_data = CheckpointData(
            workflow_id=workflow_state.workflow_id,
            checkpoint_type=checkpoint_type,
            state_snapshot=workflow_state
        )
        
        return self.save_checkpoint(checkpoint_data)
    
    async def save_workflow_state_async(
        self,
        workflow_state: WorkflowState,
        checkpoint_type: str = "periodic"
    ) -> str:
        """
        ì›Œí¬í”Œë¡œìš° ìƒíƒœ ìŠ¤ëƒ…ìƒ· ì €ì¥ (ë¹„ë™ê¸°)
        """
        return self.save_workflow_state(workflow_state, checkpoint_type)
    
    def get_workflow_state(self, workflow_id: str) -> Optional[WorkflowState]:
        """ì›Œí¬í”Œë¡œìš° ìƒíƒœ ì¡°íšŒ - ê¸°ì¡´ API ì™„ì „ í˜¸í™˜"""
        checkpoint = self.load_checkpoint(workflow_id)
        return checkpoint.state_snapshot if checkpoint else None
    
    async def get_workflow_state_async(self, workflow_id: str) -> Optional[WorkflowState]:
        """ì›Œí¬í”Œë¡œìš° ìƒíƒœ ì¡°íšŒ (ë¹„ë™ê¸°)"""
        return self.get_workflow_state(workflow_id)
    
    def cleanup_expired_checkpoints(self) -> int:
        """ë§Œë£Œëœ ì²´í¬í¬ì¸íŠ¸ ì •ë¦¬ - diskcacheê°€ ìë™ ì²˜ë¦¬"""
        # diskcacheëŠ” TTLì„ ìë™ìœ¼ë¡œ ì²˜ë¦¬í•˜ë¯€ë¡œ ìˆ˜ë™ ì •ë¦¬ ë¶ˆí•„ìš”
        # ëŒ€ì‹  í†µê³„ë§Œ ë°˜í™˜
        return 0
    
    def _load_all_checkpoints_for_workflow(self, workflow_id: str) -> List[CheckpointData]:
        """ì›Œí¬í”Œë¡œìš°ë³„ ëª¨ë“  ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ (ë‚´ë¶€ í—¬í¼)"""
        # CacheManagerì˜ get_checkpoints_by_workflow ì‚¬ìš©
        return self.cache_manager.get_checkpoints_by_workflow(workflow_id)
    
    # =============================================================================
    # API ì§€ì›ì„ ìœ„í•œ í™•ì¥ ë©”ì„œë“œë“¤ - CacheManager ê¸°ë°˜ìœ¼ë¡œ ì¬êµ¬í˜„
    # =============================================================================
    
    async def get_checkpoints_by_workflow(
        self, 
        workflow_id: str, 
        checkpoint_type: Optional[str] = None,
        limit: int = 10
    ) -> List[CheckpointData]:
        """
        íŠ¹ì • ì›Œí¬í”Œë¡œìš°ì˜ ì²´í¬í¬ì¸íŠ¸ ëª©ë¡ ì¡°íšŒ - ê¸°ì¡´ API ì™„ì „ í˜¸í™˜
        """
        try:
            # CacheManager ë°©ì‹ìœ¼ë¡œ ì¡°íšŒ
            all_checkpoints = self.cache_manager.get_checkpoints_by_workflow(workflow_id, limit)
            
            # checkpoint_type í•„í„°ë§
            if checkpoint_type:
                filtered_checkpoints = [
                    cp for cp in all_checkpoints 
                    if cp.checkpoint_type == checkpoint_type
                ]
                return filtered_checkpoints[:limit]
            
            return all_checkpoints[:limit]
            
        except Exception as e:
            logger.error(f"ì²´í¬í¬ì¸íŠ¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []
    
    async def get_latest_checkpoint(
        self, 
        workflow_id: str, 
        checkpoint_type: Optional[str] = None
    ) -> Optional[CheckpointData]:
        """
        ìµœì‹  ì²´í¬í¬ì¸íŠ¸ ì¡°íšŒ - ê¸°ì¡´ API ì™„ì „ í˜¸í™˜
        """
        try:
            checkpoints = await self.get_checkpoints_by_workflow(
                workflow_id, checkpoint_type, limit=1
            )
            return checkpoints[0] if checkpoints else None
            
        except Exception as e:
            logger.error(f"ìµœì‹  ì²´í¬í¬ì¸íŠ¸ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return None
    
    async def delete_checkpoints(
        self, 
        workflow_id: str, 
        checkpoint_type: Optional[str] = None
    ) -> int:
        """
        ì²´í¬í¬ì¸íŠ¸ ì‚­ì œ - ê¸°ì¡´ API í˜¸í™˜
        """
        try:
            # ì‚­ì œí•  ì²´í¬í¬ì¸íŠ¸ ëª©ë¡ ì¡°íšŒ
            checkpoints = await self.get_checkpoints_by_workflow(workflow_id, checkpoint_type)
            
            deleted_count = 0
            for checkpoint in checkpoints:
                success = self.cache_manager.delete_checkpoint(checkpoint.checkpoint_id)
                if success:
                    deleted_count += 1
            
            # latest í‚¤ë„ ì‚­ì œ (ì „ì²´ ì›Œí¬í”Œë¡œìš° ì‚­ì œ ì‹œ)
            if not checkpoint_type:
                latest_key = self._generate_latest_key(workflow_id)
                self.cache_manager.delete(latest_key)
            
            logger.info(f"ì›Œí¬í”Œë¡œìš° {workflow_id} ì²´í¬í¬ì¸íŠ¸ {deleted_count}ê°œ ì‚­ì œ")
            return deleted_count
            
        except Exception as e:
            logger.error(f"ì²´í¬í¬ì¸íŠ¸ ì‚­ì œ ì‹¤íŒ¨: {e}")
            return 0
    
    async def list_all_checkpoints(
        self, 
        page: int = 1, 
        page_size: int = 20,
        checkpoint_type: Optional[str] = None
    ) -> Tuple[List[CheckpointData], int]:
        """
        ì „ì²´ ì²´í¬í¬ì¸íŠ¸ ëª©ë¡ ì¡°íšŒ (í˜ì´ì§€ë„¤ì´ì…˜) - ê¸°ì¡´ API í˜¸í™˜
        """
        try:
            # ëª¨ë“  ì›Œí¬í”Œë¡œìš°ì˜ ì²´í¬í¬ì¸íŠ¸ ì¡°íšŒëŠ” diskcache íŠ¹ì„±ìƒ ì œí•œì 
            # ë‹¨ìˆœ êµ¬í˜„ìœ¼ë¡œ ëŒ€ì²´
            all_checkpoints = []
            
            # í˜ì´ì§€ë„¤ì´ì…˜ ì ìš©
            start_idx = (page - 1) * page_size
            end_idx = start_idx + page_size
            page_checkpoints = all_checkpoints[start_idx:end_idx]
            
            total = len(all_checkpoints)
            
            logger.info(f"ì „ì²´ ì²´í¬í¬ì¸íŠ¸ ëª©ë¡ ì¡°íšŒ: {len(page_checkpoints)}/{total}")
            return page_checkpoints, total
            
        except Exception as e:
            logger.error(f"ì „ì²´ ì²´í¬í¬ì¸íŠ¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return [], 0
    
    async def health_check(self) -> bool:
        """
        ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸ - ê¸°ì¡´ API í˜¸í™˜
        """
        try:
            # CacheManager ìƒíƒœ í™•ì¸
            cache_health = self.cache_manager.health_check()
            is_healthy = cache_health["status"] == "healthy"
            
            # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì €ì¥/ì¡°íšŒ
            test_key = f"{self.key_prefix}:health:test"
            test_data = {"timestamp": datetime.utcnow().isoformat()}
            
            self.cache_manager.set(test_key, json.dumps(test_data), ttl=60)
            retrieved_data = self.cache_manager.get(test_key)
            self.cache_manager.delete(test_key)
            
            is_healthy = is_healthy and (retrieved_data is not None)
            
            logger.info(f"í—¬ìŠ¤ì²´í¬ ê²°ê³¼: {'ì •ìƒ' if is_healthy else 'ë¹„ì •ìƒ'}")
            return is_healthy
            
        except Exception as e:
            logger.error(f"í—¬ìŠ¤ì²´í¬ ì‹¤íŒ¨: {e}")
            return False


class StorageManager:
    """
    í†µí•© ì €ì¥ì†Œ ê´€ë¦¬ì
    ê¸°ì¡´ RedisManager + SnapshotManagerì˜ ëª¨ë“  ê¸°ëŠ¥ì„ í†µí•©
    """
    
    def __init__(self, config: Optional[RedisConfig] = None):
        # ê¸°ì¡´ RedisConfig ë˜ëŠ” ê¸°ë³¸ê°’ ì‚¬ìš©
        redis_config = config or RedisConfig()
        
        # í™˜ê²½ë³€ìˆ˜ ê¸°ë°˜ ìºì‹œ ì„¤ì • ì ìš©
        cache_config = CacheConfig.from_env()
        
        # ë‚´ë¶€ ë§¤ë‹ˆì €ë“¤ ì´ˆê¸°í™”
        self.redis_manager = RedisManager(redis_config)
        self.redis_manager.cache_config = cache_config
        self.redis_manager.cache_manager = CacheManager(cache_config)
        self.snapshot_manager = SnapshotManager(self.redis_manager)
        self.lock_manager = DistributedLockManager()  # í™˜ê²½ë³€ìˆ˜ ìë™ ë¡œë“œ
        
        # ì§ì ‘ ì ‘ê·¼ìš© ë§¤ë‹ˆì €ë“¤
        self.cache_manager = self.redis_manager.cache_manager
        
        logger.info("StorageManager initialized - Redis functionality replaced with diskcache")
    
    # RedisManager ë©”ì„œë“œë“¤ ìœ„ì„
    def get_sync_client(self):
        """ë™ê¸° í´ë¼ì´ì–¸íŠ¸ ë°˜í™˜ (FakeRedis ë˜ëŠ” None)"""
        return self.redis_manager.get_sync_client()
    
    async def get_async_client(self):
        """ë¹„ë™ê¸° í´ë¼ì´ì–¸íŠ¸ ë°˜í™˜"""
        return await self.redis_manager.get_async_client()
    
    def test_connection(self) -> bool:
        """ì—°ê²° í…ŒìŠ¤íŠ¸"""
        return self.redis_manager.test_connection()
    
    async def test_async_connection(self) -> bool:
        """ë¹„ë™ê¸° ì—°ê²° í…ŒìŠ¤íŠ¸"""
        return await self.redis_manager.test_async_connection()
    
    # SnapshotManager ë©”ì„œë“œë“¤ ìœ„ì„
    def save_checkpoint(self, checkpoint_data: CheckpointData, ttl: Optional[int] = None) -> str:
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
        return self.snapshot_manager.save_checkpoint(checkpoint_data, ttl)
    
    async def save_checkpoint_async(self, checkpoint_data: CheckpointData, ttl: Optional[int] = None) -> str:
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (ë¹„ë™ê¸°)"""
        return await self.snapshot_manager.save_checkpoint_async(checkpoint_data, ttl)
    
    def load_checkpoint(self, workflow_id: str, latest: bool = True) -> Optional[CheckpointData]:
        """ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ"""
        return self.snapshot_manager.load_checkpoint(workflow_id, latest)
    
    async def load_checkpoint_async(self, workflow_id: str, latest: bool = True) -> Optional[CheckpointData]:
        """ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ (ë¹„ë™ê¸°)"""
        return await self.snapshot_manager.load_checkpoint_async(workflow_id, latest)
    
    def save_workflow_state(self, workflow_state: WorkflowState, checkpoint_type: str = "periodic") -> str:
        """ì›Œí¬í”Œë¡œìš° ìƒíƒœ ì €ì¥"""
        return self.snapshot_manager.save_workflow_state(workflow_state, checkpoint_type)
    
    async def save_workflow_state_async(self, workflow_state: WorkflowState, checkpoint_type: str = "periodic") -> str:
        """ì›Œí¬í”Œë¡œìš° ìƒíƒœ ì €ì¥ (ë¹„ë™ê¸°)"""
        return await self.snapshot_manager.save_workflow_state_async(workflow_state, checkpoint_type)
    
    def get_workflow_state(self, workflow_id: str) -> Optional[WorkflowState]:
        """ì›Œí¬í”Œë¡œìš° ìƒíƒœ ì¡°íšŒ"""
        return self.snapshot_manager.get_workflow_state(workflow_id)
    
    async def get_workflow_state_async(self, workflow_id: str) -> Optional[WorkflowState]:
        """ì›Œí¬í”Œë¡œìš° ìƒíƒœ ì¡°íšŒ (ë¹„ë™ê¸°)"""
        return await self.snapshot_manager.get_workflow_state_async(workflow_id)
    
    # í™•ì¥ API ë©”ì„œë“œë“¤ ìœ„ì„
    async def get_checkpoints_by_workflow(self, workflow_id: str, checkpoint_type: Optional[str] = None, limit: int = 10) -> List[CheckpointData]:
        """ì›Œí¬í”Œë¡œìš°ë³„ ì²´í¬í¬ì¸íŠ¸ ëª©ë¡ ì¡°íšŒ"""
        return await self.snapshot_manager.get_checkpoints_by_workflow(workflow_id, checkpoint_type, limit)
    
    async def get_latest_checkpoint(self, workflow_id: str, checkpoint_type: Optional[str] = None) -> Optional[CheckpointData]:
        """ìµœì‹  ì²´í¬í¬ì¸íŠ¸ ì¡°íšŒ"""
        return await self.snapshot_manager.get_latest_checkpoint(workflow_id, checkpoint_type)
    
    async def delete_checkpoints(self, workflow_id: str, checkpoint_type: Optional[str] = None) -> int:
        """ì²´í¬í¬ì¸íŠ¸ ì‚­ì œ"""
        return await self.snapshot_manager.delete_checkpoints(workflow_id, checkpoint_type)
    
    async def list_all_checkpoints(self, page: int = 1, page_size: int = 20, checkpoint_type: Optional[str] = None) -> Tuple[List[CheckpointData], int]:
        """ì „ì²´ ì²´í¬í¬ì¸íŠ¸ ëª©ë¡ ì¡°íšŒ"""
        return await self.snapshot_manager.list_all_checkpoints(page, page_size, checkpoint_type)
    
    def cleanup_expired_checkpoints(self) -> int:
        """ë§Œë£Œëœ ì²´í¬í¬ì¸íŠ¸ ì •ë¦¬"""
        return self.snapshot_manager.cleanup_expired_checkpoints()
    
    # ë¶„ì‚° ë½ ë©”ì„œë“œë“¤
    def acquire_lock(self, resource_name: str, ttl: int = 30, blocking: bool = True, timeout: Optional[float] = None):
        """ë¶„ì‚° ë½ íšë“"""
        return self.lock_manager.acquire_lock(resource_name, ttl, blocking, timeout)
    
    def acquire_lock_sync(self, resource_name: str, ttl: int = 30, timeout: Optional[float] = None) -> Optional[str]:
        """ë™ê¸° ë½ íšë“"""
        return self.lock_manager.acquire_lock_sync(resource_name, ttl, timeout)
    
    def release_lock(self, resource_name: str, lock_id: str) -> bool:
        """ë½ í•´ì œ"""
        return self.lock_manager.release_lock(resource_name, lock_id)
    
    def is_locked(self, resource_name: str) -> bool:
        """ë½ ìƒíƒœ í™•ì¸"""
        return self.lock_manager.is_locked(resource_name)
    
    # ìƒíƒœ ì ê²€
    async def health_check(self) -> Dict[str, Any]:
        """ì „ì²´ ì‹œìŠ¤í…œ ìƒíƒœ ì ê²€"""
        try:
            # ê° ì»´í¬ë„ŒíŠ¸ ìƒíƒœ í™•ì¸
            cache_health = self.cache_manager.health_check()
            lock_health = self.lock_manager.health_check()
            snapshot_health = await self.snapshot_manager.health_check()
            
            return {
                "status": "healthy" if all([
                    cache_health["status"] == "healthy",
                    lock_health["status"] == "healthy",
                    snapshot_health
                ]) else "unhealthy",
                "components": {
                    "cache_manager": cache_health,
                    "lock_manager": lock_health,
                    "snapshot_manager": snapshot_health,
                    "redis_compatibility": FAKEREDIS_AVAILABLE
                },
                "migration_info": {
                    "redis_replaced": True,
                    "backend": "diskcache + filelock",
                    "compatibility_mode": FAKEREDIS_AVAILABLE
                }
            }
        except Exception as e:
            return {"status": "error", "error": str(e)}
    
    def close(self):
        """ì—°ê²° ì¢…ë£Œ"""
        self.redis_manager.close()
    
    async def close_async(self):
        """ë¹„ë™ê¸° ì—°ê²° ì¢…ë£Œ"""
        await self.redis_manager.close_async()


if __name__ == "__main__":
    # ê¸°ë³¸ í…ŒìŠ¤íŠ¸
    import asyncio
    
    async def test_storage_manager():
        manager = StorageManager()
        print("âœ… StorageManager ì™„ì „ êµ¬í˜„ ì„±ê³µ")
        
        health = await manager.health_check()
        print("ğŸ“Š ì „ì²´ ìƒíƒœ:", json.dumps(health, indent=2))
        
        await manager.close_async()
    
    asyncio.run(test_storage_manager())