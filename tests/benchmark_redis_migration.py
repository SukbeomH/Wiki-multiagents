"""
Redis ë§ˆì´ê·¸ë ˆì´ì…˜ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

Redis vs diskcache ì„±ëŠ¥ ë¹„êµ ë° ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸
"""

import time
import statistics
import asyncio
import tempfile
import shutil
import json
from datetime import datetime, timedelta
from pathlib import Path
from typing import List, Dict, Any
from dataclasses import dataclass

from server.utils.storage_manager import StorageManager
from server.utils.cache_manager import CacheManager, CacheConfig
from server.utils.lock_manager import DistributedLockManager
from server.schemas.base import CheckpointData, CheckpointType, WorkflowState, WorkflowStage


@dataclass
class BenchmarkResult:
    """ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼"""
    operation: str
    iterations: int
    total_time: float
    avg_time: float
    min_time: float
    max_time: float
    median_time: float
    operations_per_second: float
    success_rate: float


class RedisMigrationBenchmark:
    """Redis ë§ˆì´ê·¸ë ˆì´ì…˜ ë²¤ì¹˜ë§ˆí¬"""
    
    def __init__(self, temp_dir: str = None):
        self.temp_dir = temp_dir or tempfile.mkdtemp()
        self.results: List[BenchmarkResult] = []
        self.setup_storage()
    
    def setup_storage(self):
        """ìŠ¤í† ë¦¬ì§€ ì‹œìŠ¤í…œ ì„¤ì •"""
        import os
        
        # í™˜ê²½ë³€ìˆ˜ ì„¤ì •
        os.environ.update({
            "CACHE_DIR": f"{self.temp_dir}/cache",
            "LOCK_DIR": f"{self.temp_dir}/locks",
            "CACHE_MAX_SIZE": str(100 * 1024 * 1024),  # 100MB
            "CACHE_DEFAULT_TTL": "3600",  # 1ì‹œê°„
            "CACHE_CHECKPOINT_TTL": "86400"  # 24ì‹œê°„
        })
        
        self.storage_manager = StorageManager()
        print(f"âœ… ë²¤ì¹˜ë§ˆí¬ í™˜ê²½ ì„¤ì • ì™„ë£Œ: {self.temp_dir}")
    
    def cleanup(self):
        """ì •ë¦¬"""
        try:
            shutil.rmtree(self.temp_dir, ignore_errors=True)
            print(f"ğŸ§¹ ë²¤ì¹˜ë§ˆí¬ í™˜ê²½ ì •ë¦¬ ì™„ë£Œ")
        except Exception as e:
            print(f"âš ï¸ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def measure_time(self, func, *args, **kwargs) -> tuple:
        """ì‹¤í–‰ ì‹œê°„ ì¸¡ì •"""
        start_time = time.time()
        try:
            result = func(*args, **kwargs)
            success = True
        except Exception as e:
            result = None
            success = False
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        
        end_time = time.time()
        return end_time - start_time, success, result
    
    def benchmark_cache_operations(self, iterations: int = 1000):
        """ìºì‹œ ì‘ì—… ë²¤ì¹˜ë§ˆí¬"""
        print(f"\nğŸš€ ìºì‹œ ì‘ì—… ë²¤ì¹˜ë§ˆí¬ ì‹œì‘ ({iterations}íšŒ)")
        
        # ê¸°ë³¸ Set/Get ë²¤ì¹˜ë§ˆí¬
        set_times = []
        get_times = []
        successful_operations = 0
        
        for i in range(iterations):
            key = f"benchmark_key_{i}"
            value = f"benchmark_value_{i}_{'x' * 100}"  # ~100ì ë°ì´í„°
            
            # Set ì„±ëŠ¥ ì¸¡ì •
            set_time, success, _ = self.measure_time(
                self.storage_manager.cache_manager.set, key, value
            )
            set_times.append(set_time)
            
            if success:
                # Get ì„±ëŠ¥ ì¸¡ì •
                get_time, success, retrieved = self.measure_time(
                    self.storage_manager.cache_manager.get, key
                )
                get_times.append(get_time)
                
                if success and retrieved == value:
                    successful_operations += 1
        
        # Set ê²°ê³¼ ì €ì¥
        self.results.append(BenchmarkResult(
            operation="Cache Set",
            iterations=iterations,
            total_time=sum(set_times),
            avg_time=statistics.mean(set_times),
            min_time=min(set_times),
            max_time=max(set_times),
            median_time=statistics.median(set_times),
            operations_per_second=iterations / sum(set_times),
            success_rate=successful_operations / iterations
        ))
        
        # Get ê²°ê³¼ ì €ì¥
        self.results.append(BenchmarkResult(
            operation="Cache Get",
            iterations=iterations,
            total_time=sum(get_times),
            avg_time=statistics.mean(get_times),
            min_time=min(get_times),
            max_time=max(get_times),
            median_time=statistics.median(get_times),
            operations_per_second=iterations / sum(get_times),
            success_rate=successful_operations / iterations
        ))
        
        print(f"âœ… ìºì‹œ Set: {iterations / sum(set_times):.2f} ops/sec")
        print(f"âœ… ìºì‹œ Get: {iterations / sum(get_times):.2f} ops/sec")
    
    def benchmark_json_operations(self, iterations: int = 500):
        """JSON ì‘ì—… ë²¤ì¹˜ë§ˆí¬"""
        print(f"\nğŸš€ JSON ì‘ì—… ë²¤ì¹˜ë§ˆí¬ ì‹œì‘ ({iterations}íšŒ)")
        
        json_set_times = []
        json_get_times = []
        successful_operations = 0
        
        for i in range(iterations):
            key = f"json_benchmark_{i}"
            json_data = {
                "id": i,
                "timestamp": datetime.now().isoformat(),
                "data": {
                    "items": [f"item_{j}" for j in range(10)],
                    "metadata": {
                        "type": "benchmark",
                        "iteration": i,
                        "size": "medium"
                    }
                },
                "status": "active"
            }
            
            # JSON Set ì„±ëŠ¥ ì¸¡ì •
            set_time, success, _ = self.measure_time(
                self.storage_manager.cache_manager.json_set, key, "$", json_data
            )
            json_set_times.append(set_time)
            
            if success:
                # JSON Get ì„±ëŠ¥ ì¸¡ì •
                get_time, success, retrieved = self.measure_time(
                    self.storage_manager.cache_manager.json_get, key, "$"
                )
                json_get_times.append(get_time)
                
                if success and retrieved == json_data:
                    successful_operations += 1
        
        # JSON Set ê²°ê³¼
        self.results.append(BenchmarkResult(
            operation="JSON Set",
            iterations=iterations,
            total_time=sum(json_set_times),
            avg_time=statistics.mean(json_set_times),
            min_time=min(json_set_times),
            max_time=max(json_set_times),
            median_time=statistics.median(json_set_times),
            operations_per_second=iterations / sum(json_set_times),
            success_rate=successful_operations / iterations
        ))
        
        # JSON Get ê²°ê³¼
        self.results.append(BenchmarkResult(
            operation="JSON Get",
            iterations=iterations,
            total_time=sum(json_get_times),
            avg_time=statistics.mean(json_get_times),
            min_time=min(json_get_times),
            max_time=max(json_get_times),
            median_time=statistics.median(json_get_times),
            operations_per_second=iterations / sum(json_get_times),
            success_rate=successful_operations / iterations
        ))
        
        print(f"âœ… JSON Set: {iterations / sum(json_set_times):.2f} ops/sec")
        print(f"âœ… JSON Get: {iterations / sum(json_get_times):.2f} ops/sec")
    
    def benchmark_lock_operations(self, iterations: int = 200):
        """ë½ ì‘ì—… ë²¤ì¹˜ë§ˆí¬"""
        print(f"\nğŸš€ ë½ ì‘ì—… ë²¤ì¹˜ë§ˆí¬ ì‹œì‘ ({iterations}íšŒ)")
        
        acquire_times = []
        release_times = []
        successful_operations = 0
        
        for i in range(iterations):
            resource = f"benchmark_lock_{i}"
            
            # ë½ íšë“ ì„±ëŠ¥ ì¸¡ì •
            acquire_time, success, lock_id = self.measure_time(
                self.storage_manager.acquire_lock_sync, resource, 5, 1.0
            )
            acquire_times.append(acquire_time)
            
            if success and lock_id:
                # ë½ í•´ì œ ì„±ëŠ¥ ì¸¡ì •
                release_time, success, _ = self.measure_time(
                    self.storage_manager.release_lock, resource, lock_id
                )
                release_times.append(release_time)
                
                if success:
                    successful_operations += 1
        
        # ë½ íšë“ ê²°ê³¼
        self.results.append(BenchmarkResult(
            operation="Lock Acquire",
            iterations=iterations,
            total_time=sum(acquire_times),
            avg_time=statistics.mean(acquire_times),
            min_time=min(acquire_times),
            max_time=max(acquire_times),
            median_time=statistics.median(acquire_times),
            operations_per_second=iterations / sum(acquire_times),
            success_rate=successful_operations / iterations
        ))
        
        # ë½ í•´ì œ ê²°ê³¼
        if release_times:
            self.results.append(BenchmarkResult(
                operation="Lock Release",
                iterations=len(release_times),
                total_time=sum(release_times),
                avg_time=statistics.mean(release_times),
                min_time=min(release_times),
                max_time=max(release_times),
                median_time=statistics.median(release_times),
                operations_per_second=len(release_times) / sum(release_times),
                success_rate=successful_operations / iterations
            ))
        
        print(f"âœ… ë½ Acquire: {iterations / sum(acquire_times):.2f} ops/sec")
        if release_times:
            print(f"âœ… ë½ Release: {len(release_times) / sum(release_times):.2f} ops/sec")
    
    def benchmark_checkpoint_operations(self, iterations: int = 100):
        """ì²´í¬í¬ì¸íŠ¸ ì‘ì—… ë²¤ì¹˜ë§ˆí¬"""
        print(f"\nğŸš€ ì²´í¬í¬ì¸íŠ¸ ì‘ì—… ë²¤ì¹˜ë§ˆí¬ ì‹œì‘ ({iterations}íšŒ)")
        
        save_times = []
        load_times = []
        successful_operations = 0
        
        for i in range(iterations):
            workflow_id = f"benchmark_workflow_{i}"
            
            # WorkflowState ìƒì„±
            workflow_state = WorkflowState(
                workflow_id=workflow_id,
                trace_id=f"benchmark_trace_{i}",
                current_stage=WorkflowStage.RESEARCH,
                keyword=f"benchmark test {i}"
            )
            
            # CheckpointData ìƒì„±
            checkpoint_data = CheckpointData(
                workflow_id=workflow_id,
                checkpoint_type=CheckpointType.PERIODIC,
                state_snapshot=workflow_state
            )
            
            # ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì„±ëŠ¥ ì¸¡ì •
            save_time, success, checkpoint_key = self.measure_time(
                self.storage_manager.save_checkpoint, checkpoint_data
            )
            save_times.append(save_time)
            
            if success and checkpoint_key:
                # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì„±ëŠ¥ ì¸¡ì •
                load_time, success, loaded = self.measure_time(
                    self.storage_manager.load_checkpoint, workflow_id
                )
                load_times.append(load_time)
                
                if success and loaded and loaded.workflow_id == workflow_id:
                    successful_operations += 1
        
        # ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ê²°ê³¼
        self.results.append(BenchmarkResult(
            operation="Checkpoint Save",
            iterations=iterations,
            total_time=sum(save_times),
            avg_time=statistics.mean(save_times),
            min_time=min(save_times),
            max_time=max(save_times),
            median_time=statistics.median(save_times),
            operations_per_second=iterations / sum(save_times),
            success_rate=successful_operations / iterations
        ))
        
        # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ê²°ê³¼
        if load_times:
            self.results.append(BenchmarkResult(
                operation="Checkpoint Load",
                iterations=len(load_times),
                total_time=sum(load_times),
                avg_time=statistics.mean(load_times),
                min_time=min(load_times),
                max_time=max(load_times),
                median_time=statistics.median(load_times),
                operations_per_second=len(load_times) / sum(load_times),
                success_rate=successful_operations / iterations
            ))
        
        print(f"âœ… ì²´í¬í¬ì¸íŠ¸ Save: {iterations / sum(save_times):.2f} ops/sec")
        if load_times:
            print(f"âœ… ì²´í¬í¬ì¸íŠ¸ Load: {len(load_times) / sum(load_times):.2f} ops/sec")
    
    def benchmark_concurrent_operations(self, workers: int = 5, operations_per_worker: int = 50):
        """ë™ì‹œì„± ì‘ì—… ë²¤ì¹˜ë§ˆí¬"""
        print(f"\nğŸš€ ë™ì‹œì„± ì‘ì—… ë²¤ì¹˜ë§ˆí¬ ì‹œì‘ ({workers} ì›Œì»¤, ê° {operations_per_worker}íšŒ)")
        
        import threading
        import queue
        
        results_queue = queue.Queue()
        start_time = time.time()
        
        def worker_task(worker_id):
            """ì›Œì»¤ ì‘ì—…"""
            worker_times = []
            successful_ops = 0
            
            for i in range(operations_per_worker):
                resource = f"concurrent_test_{worker_id}_{i}"
                
                # ë½ íšë“/í•´ì œ ì‚¬ì´í´
                op_start = time.time()
                
                with self.storage_manager.acquire_lock(resource, ttl=1) as lock_id:
                    if lock_id:
                        # ìºì‹œ ì‘ì—…
                        key = f"worker_{worker_id}_data_{i}"
                        data = {"worker": worker_id, "operation": i, "timestamp": time.time()}
                        
                        self.storage_manager.cache_manager.json_set(key, "$", data)
                        retrieved = self.storage_manager.cache_manager.json_get(key, "$")
                        
                        if retrieved == data:
                            successful_ops += 1
                
                worker_times.append(time.time() - op_start)
            
            results_queue.put((worker_id, worker_times, successful_ops))
        
        # ì›Œì»¤ ìŠ¤ë ˆë“œ ì‹œì‘
        threads = []
        for worker_id in range(workers):
            thread = threading.Thread(target=worker_task, args=(worker_id,))
            threads.append(thread)
            thread.start()
        
        # ëª¨ë“  ìŠ¤ë ˆë“œ ì™„ë£Œ ëŒ€ê¸°
        for thread in threads:
            thread.join()
        
        # ê²°ê³¼ ìˆ˜ì§‘
        total_time = time.time() - start_time
        all_times = []
        total_successful = 0
        
        while not results_queue.empty():
            worker_id, times, successful = results_queue.get()
            all_times.extend(times)
            total_successful += successful
        
        total_operations = workers * operations_per_worker
        
        # ë™ì‹œì„± ê²°ê³¼
        self.results.append(BenchmarkResult(
            operation="Concurrent Operations",
            iterations=total_operations,
            total_time=total_time,
            avg_time=statistics.mean(all_times),
            min_time=min(all_times),
            max_time=max(all_times),
            median_time=statistics.median(all_times),
            operations_per_second=total_operations / total_time,
            success_rate=total_successful / total_operations
        ))
        
        print(f"âœ… ë™ì‹œì„± ì‘ì—…: {total_operations / total_time:.2f} ops/sec")
        print(f"âœ… ì„±ê³µë¥ : {(total_successful / total_operations) * 100:.1f}%")
    
    def generate_report(self) -> Dict[str, Any]:
        """ë²¤ì¹˜ë§ˆí¬ ë³´ê³ ì„œ ìƒì„±"""
        report = {
            "timestamp": datetime.now().isoformat(),
            "system_info": {
                "backend": "diskcache + filelock",
                "temp_dir": self.temp_dir,
                "storage_type": "local_filesystem"
            },
            "benchmark_results": [],
            "summary": {}
        }
        
        # ê°œë³„ ê²°ê³¼ ì¶”ê°€
        for result in self.results:
            report["benchmark_results"].append({
                "operation": result.operation,
                "iterations": result.iterations,
                "total_time": round(result.total_time, 4),
                "avg_time": round(result.avg_time, 6),
                "min_time": round(result.min_time, 6),
                "max_time": round(result.max_time, 6),
                "median_time": round(result.median_time, 6),
                "operations_per_second": round(result.operations_per_second, 2),
                "success_rate": round(result.success_rate, 4)
            })
        
        # ìš”ì•½ í†µê³„
        if self.results:
            total_operations = sum(r.iterations for r in self.results)
            total_time = sum(r.total_time for r in self.results)
            avg_ops_per_sec = statistics.mean([r.operations_per_second for r in self.results])
            avg_success_rate = statistics.mean([r.success_rate for r in self.results])
            
            report["summary"] = {
                "total_operations": total_operations,
                "total_time": round(total_time, 4),
                "average_ops_per_second": round(avg_ops_per_sec, 2),
                "average_success_rate": round(avg_success_rate, 4),
                "fastest_operation": max(self.results, key=lambda r: r.operations_per_second).operation,
                "slowest_operation": min(self.results, key=lambda r: r.operations_per_second).operation
            }
        
        return report
    
    def print_report(self):
        """ë³´ê³ ì„œ ì¶œë ¥"""
        print("\n" + "="*80)
        print("ğŸ† Redis ë§ˆì´ê·¸ë ˆì´ì…˜ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ë³´ê³ ì„œ")
        print("="*80)
        
        if not self.results:
            print("âŒ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print(f"\nğŸ“Š ì‹œìŠ¤í…œ ì •ë³´:")
        print(f"   ë°±ì—”ë“œ: diskcache + filelock")
        print(f"   ì €ì¥ì†Œ: {self.temp_dir}")
        print(f"   ë‚ ì§œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        print(f"\nğŸ“ˆ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼:")
        print("-"*80)
        print(f"{'ì‘ì—…':<20} {'íšŸìˆ˜':<8} {'ì´ ì‹œê°„':<10} {'í‰ê· ':<12} {'ops/sec':<12} {'ì„±ê³µë¥ ':<8}")
        print("-"*80)
        
        for result in self.results:
            print(f"{result.operation:<20} "
                  f"{result.iterations:<8} "
                  f"{result.total_time:<10.4f} "
                  f"{result.avg_time:<12.6f} "
                  f"{result.operations_per_second:<12.2f} "
                  f"{result.success_rate:<8.4f}")
        
        # ìš”ì•½
        total_ops = sum(r.iterations for r in self.results)
        total_time = sum(r.total_time for r in self.results)
        avg_ops_per_sec = statistics.mean([r.operations_per_second for r in self.results])
        
        print("-"*80)
        print(f"ğŸ“‹ ìš”ì•½:")
        print(f"   ì´ ì‘ì—… ìˆ˜: {total_ops:,}")
        print(f"   ì´ ì‹¤í–‰ ì‹œê°„: {total_time:.4f}ì´ˆ")
        print(f"   í‰ê·  ì²˜ë¦¬ëŸ‰: {avg_ops_per_sec:.2f} ops/sec")
        print(f"   ê°€ì¥ ë¹ ë¥¸ ì‘ì—…: {max(self.results, key=lambda r: r.operations_per_second).operation}")
        print(f"   ê°€ì¥ ëŠë¦° ì‘ì—…: {min(self.results, key=lambda r: r.operations_per_second).operation}")
        print("="*80)
    
    def save_report(self, filename: str = None):
        """ë³´ê³ ì„œ íŒŒì¼ ì €ì¥"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"redis_migration_benchmark_{timestamp}.json"
        
        report = self.generate_report()
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ’¾ ë²¤ì¹˜ë§ˆí¬ ë³´ê³ ì„œ ì €ì¥: {filename}")
        return filename


def run_full_benchmark():
    """ì „ì²´ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
    print("ğŸš€ Redis ë§ˆì´ê·¸ë ˆì´ì…˜ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘")
    print("="*80)
    
    benchmark = RedisMigrationBenchmark()
    
    try:
        # ê°ì¢… ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
        benchmark.benchmark_cache_operations(iterations=1000)
        benchmark.benchmark_json_operations(iterations=500)
        benchmark.benchmark_lock_operations(iterations=200)
        benchmark.benchmark_checkpoint_operations(iterations=100)
        benchmark.benchmark_concurrent_operations(workers=5, operations_per_worker=50)
        
        # ê²°ê³¼ ì¶œë ¥ ë° ì €ì¥
        benchmark.print_report()
        report_file = benchmark.save_report()
        
        print(f"\nâœ… ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ! ë³´ê³ ì„œ: {report_file}")
        
    except Exception as e:
        print(f"âŒ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        benchmark.cleanup()


if __name__ == "__main__":
    run_full_benchmark()